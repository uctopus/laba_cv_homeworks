{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10 - Convolutional Neural Networks 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fill out the missing parts of the code. This time you will be building a simple convolutional neural network.\n",
    "\n",
    "This will be a the first time that you will build a simple neural network using the torch library.\n",
    "I suggest first to solve the task on MNIST dataset (28x28x1 images) and then on CIFAR-10 dataset (32x32x3 images).\n",
    "It's better to first try on MNIST, so you can iterate faster on the solution, and then move to CIFAR-10.\n",
    "- You are expected to reach at least 90% accuracy on the test set for MNIST (use_cifar=False).\n",
    "- If you succeed on MNIST, you are expected to reach at least 40% accuracy on the test set for CIFAR-10 (use_cifar=True). This will take potentially more\n",
    "time (depending on your hardware), but you should still be able to reach at least 40% accuracy on the test set.\n",
    "\n",
    "It is a simple task, so don't try to make the model too big (i.e. one or two convolutional layers with 4, 8 or 16 filters should be enough).\n",
    "\n",
    "Once you're done with the task, compute (estimate) the number of parameters in the model. Compare it with the number of parameters in the homework 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=64, val_size=1000, cifar=False):\n",
    "    \"\"\"\n",
    "    Loads dataset and returns DataLoader objects.\n",
    "\n",
    "    :param batch_size: Number of samples per batch\n",
    "    :param val_size: Number of samples for validation set\n",
    "    :param cifar: If True, loads CIFAR-10 dataset, otherwise MNIST\n",
    "    :returns: train_loader, test_loader, val_loader\n",
    "    \"\"\"\n",
    "    print(f\"Loading {'CIFAR-10' if cifar else 'MNIST'} dataset...\")\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "            if not cifar\n",
    "            else transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    dataset_object = (\n",
    "        torchvision.datasets.CIFAR10 if cifar else torchvision.datasets.MNIST\n",
    "    )\n",
    "\n",
    "    train_dataset = dataset_object(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = dataset_object(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [len(train_dataset) - val_size, val_size]\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    print(\n",
    "        f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\"\n",
    "    )\n",
    "    return train_loader, test_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_channels: int, output_size: int):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \"\"\"\n",
    "        A simple CNN model with batch normalization and dropout for improved efficiency.\n",
    "        \n",
    "        Feel free to use any tricks that you know, but mandatory is to use:\n",
    "        - Convolutional layer(s)\n",
    "        - Non-linear activation function\n",
    "        - Pooling layer(s)\n",
    "        - Fully connected layer\n",
    "        \n",
    "        At the end of the network you need to convert activation maps into a vector and \n",
    "        then pass it through the fully connected layer to get the class scores.\n",
    "        \n",
    "        Make this network work both with MNIST and CIFAR-10 datasets (however you like it,\n",
    "        but do not create a new network for each dataset).\n",
    "\n",
    "        :param num_channels: Number of input channels\n",
    "        :param output_size: Number of output classes\n",
    "        \"\"\"\n",
    "        # <your_code_here>\n",
    "        # </your_code_here>\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param x: Input tensor of shape (batch_size, channels, height, width)\n",
    "        :returns: Output tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # <your_code_here>\n",
    "        x = None\n",
    "        # </your_code_here>\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the MLP model on the training data.\n",
    "\n",
    "    :param model: The neural network model\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param criterion: Loss function\n",
    "    :param optimizer: Optimizer\n",
    "    :param num_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    # we need to set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    running_validation = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss_epoch = 0.0\n",
    "        running_validation_epoch = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            \"\"\"\n",
    "            Implement the training loop.\n",
    "            It should include:\n",
    "            1. Forward pass (passing the images through the model)\n",
    "            2. Loss computation (using the criterion input function)\n",
    "            3. Backward pass (computing the gradients)\n",
    "            4. Optimization step (updating the model parameters)\n",
    "            \n",
    "            Note: you will be using the torch optimizer.\n",
    "            First you need to zero the gradients in the optimizer (clear the gradients from the previous epoch)\n",
    "            Then you need to call the backward pass on the loss (compute the gradients)\n",
    "            Finally, you need to call the step function on the optimizer (to update the model parameters)\n",
    "            \"\"\"\n",
    "            # <your_code_here>\n",
    "            loss = None\n",
    "            # </your_code_here>\n",
    "\n",
    "            running_loss_epoch += loss.item()\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            \"\"\"\n",
    "            Implement the validation loop.\n",
    "            It should include:\n",
    "            1. Forward pass (passing the images through the model)\n",
    "            2. Loss computation\n",
    "            \"\"\"\n",
    "            # <your_code_here>\n",
    "            loss = None\n",
    "            # </your_code_here>\n",
    "            running_validation_epoch += loss.item()\n",
    "\n",
    "        average_loss = running_loss_epoch / len(train_loader)\n",
    "        average_validation = running_validation_epoch / len(val_loader)\n",
    "        running_loss.append(average_loss)\n",
    "        running_validation.append(average_validation)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}, Validation Loss: {average_validation:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return running_loss, running_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test data.\n",
    "\n",
    "    :param model: The neural network model\n",
    "    :param test_loader: DataLoader for test data\n",
    "    \"\"\"\n",
    "    # we need to set the model to evaluation mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy of the model on the test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cifar = True  # try on MNIST first, then on CIFAR-10 (set to True)\n",
    "# <your_code_here>\n",
    "# learning rate is the step size for the optimizer\n",
    "learning_rate = 0.001\n",
    "# the higher the learning rate, the longer the training (and the more likely to overfit)\n",
    "num_epochs = 10\n",
    "# the higher the batch_size, the faster the training, but the memory consumption is higher\n",
    "batch_size = 128\n",
    "# </your_code_here>\n",
    "output_size = 10  # MNIST has 10 classes (digits 0-9), CIFAR-10 has 10 classes (objects)\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader, val_loader = load_data(\n",
    "    batch_size=batch_size, cifar=use_cifar\n",
    ")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN(num_channels=1 if not use_cifar else 3, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "running_loss, running_validation_loss = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(running_loss, label=\"Training Loss\")\n",
    "plt.plot(running_validation_loss, label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let's visualize the weight matrix of the first layer conv1.weight\")\n",
    "print(\"Think about what the weight matrix means in this context.\")\n",
    "param = model.conv1.weight.data\n",
    "param_shape = param.shape[-1]\n",
    "filters = param.view(-1, param_shape, param_shape)\n",
    "\n",
    "num_images = param.shape[0]\n",
    "print(f\"Showing {num_images} filters\")\n",
    "num_images_columns = math.ceil(np.sqrt(num_images))\n",
    "num_images_rows = math.ceil(num_images / num_images_columns)\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_images_rows, num_images_columns, i + 1)\n",
    "    plt.imshow(param[i][0].detach().numpy(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")  # Remove tickers (axis labels and ticks)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
